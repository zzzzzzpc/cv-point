# Yolo系列

### Yolo-v1

部分参考了：http://blog.pointborn.com/article/2022/1/19/1782.html

摘要：yolo是one stage的网络，直接生成目标的位置和类别

优点：快。

缺点：对小目标效果不太好，yolov1的每个cell只能负责预测一个类别。



正文：

图片缩放到448 * 448，然后将图片分为7*7的小格。

每个格子需要预测B个框（论文里面是B=2），C个类别，需要注意的是如果一个网格当中出现了两个物体的中心，或者一个网格中包含了很多个小物体，那么YOLO算法仅能够检测出一个类别的一个物体，所以YOLO对靠的很近的物体以及小目标的检测表现不是特别好。

> 这里举个例子，假设我们的数据集有20个类别的数据，我们使用2个boundingbox（b=2），基于7*7的格子，我们最后输出的tensor应该是：7 * 7 * 2 * 5 + 7 * 7 * 20 = 7 * 7 * 30
>
> 对于类别的置信度而言（即20个每个类别的概率），计算方 法如下：
>
> from: https://www.cnblogs.com/lijie-blog/p/10180271.html
>
> ![img](https://img2018.cnblogs.com/blog/1070237/201812/1070237-20181226170651521-1428752371.png)

网络结构：

整个网络是一个端到端的网络，其整体的网络结构参考了GooLeNet的结构：

![img](https://img-blog.csdnimg.cn/20200811145153575.png#pic_center)

损失函数部分的分析：

![img](https://img-blog.csdnimg.cn/20200811150352478.png#pic_center)

- 第一部分为中心点损失，即为边界框中心坐标的误差项，1ijobj 指的是第 i 个单元格存在目标，且该单元格中的第 j 个边界框负责预测该目标；
- 第二部分为宽高损失项，即为边界框的高与宽的误差项；
- 第三部分为置信度损失项，由包含目标边界框的置信度误差项和不包含目标的边界框的置信度误差项组成；
- 最后一部分为类别损失，表示包含目标的单元格的分类误差项，1iobj 指第 i 个单元格存在目标。

在这里的两个超参数coord=5，noobj=0.5。

> 网络预测多个边界框，最后也使用了NMS

### Yolo-v2

相对于Yolov1，加入了BN层，提高了分辨率，去掉了最后的全连接层和最后一个pooling层。

**使用了Anchor Box（Convolutional With Anchor Boxes）：**

在YOLOv1当中，我们将输入分成7 * 7，每个网格预测2个bounding box，那么一共会有7 * 7 * 2一共98个bounding box。

在YOLOv2当中引入anchor boxes，输出的feature map的大小为13 * 13，每个cell当中有5个anchor box预测得到5个bounding box，那么一共有13 * 13 * 5即845个box，能够提高定位的准确度。

在anchor框的尺寸的选择上面，并没有使用像Faster-rcnn论文当中手动设置尺寸的方法，**而是使用了Kmeans聚类的方式获得anchor尺寸。**

在kmeans的距离上面，没有使用欧式距离，因为欧式距离可能会让大的bouding box相对小的bouding box产生更多的error，我们希望通过anchor boxes获得一个比较好的IOU score，所以我们的计算公式为：

```
d(box, centroid)=1-IOU(box, centroid)
```

在计算anchor boxes的时候，我们将所有的boxes中心点x，y都置为0，这样所有的boxes都处在相同的位置上，方便我们通过新的距离公式计算boxes之间的相似度。

> 参考：https://blog.csdn.net/xiaomifanhxx/article/details/81215051

**直接位置预测：**

在faster-rcnn论文当中，基于候选框的网络是通过预测相对锚框中心的偏移值来预测边界的中心坐标：

以下面这个例子为例，xa，ya以及wa，ha表示anchor的中心点位置以及宽高，t为预测的偏移量，x，y，w，h为最终的预测的结果。

![img](https://img2020.cnblogs.com/blog/1177559/202010/1177559-20201009183509917-1624413703.png)

这样的弊处在于，我们预测的框的范围可能会比较大，不利于网络收敛，例如当tx=1的时候，我们的预测结果可能直接平移了一个框的长度。

在Yolov2当中，预测边界框的中心点只相对于网格**左上角**的偏移值，每个网格有5个框来预测5个边界框，每个边界框都有5个值：tx，ty，tw，th，to。其中to表示置信度，cx，cy分别表示网格相对于左上角的坐标，锚框的先验宽度和高度为pw，ph，对应的计算公式为（b为预测的结果）：

![image-20220730163255705](D:\OneDrive\Documents\cv-point\imgs\yolov2-1.png)



**New backbone & Fine-Grained Features & Multi-scale Training**

Yolov2采用了一个新的基础模型，称之为Darknet-19，其包括19个卷积层以及5个maxpooling层。

Yolov2最后一层卷积层输出是13×13的特征图，这个分辨率对于大尺寸物体的检测已经足够，但对于小尺寸物体不一定奏效。越小的物体，经过层层池化，体现在最后特征图中的可能性越小。

作者通过添加直通层(passthrough Layer)的方式将前一层高分辨率的特征图连接到低分辨率的特征图上：前一层的特征图维度为26×26×512，最后一层的特征图的维度为13×13×1024，通过直通层将26×26×512的特征图映射到13×13×2048，然后将其与最后一层的特征图连接为13×13×（1024+2048）的特征图，最后在此特征上进行卷积预测。

在多尺度方面，因为Yolov2去掉了全连接层，所以网络理论上可以接收任意尺度的输入。在训练的时候，每10个批次从10个不同的尺寸当中随机选择一种训练（320、352、384、416、448、480、512、544、576、608）。













