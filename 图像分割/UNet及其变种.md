# UNet及其变种

### 原始Unet

paper: https://arxiv.org/abs/1505.04597v1

code: https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8 (在pytorch当中的实现)



UNet最开始为了解决细胞层面的分割任务，后来UNet或者类U型结构被广泛运用到医学相关的任务当中。

##### 网络结构图：

![UNet.png](..\imgs\UNet.png)

整个神经网络主要有两部分组成：**收缩路径（contracting path）**和**扩展路径（expanding path）**，

**（1）收缩路径（contracting path）**其实就是一个常规的卷积网络，它包含重复的2个3x3卷积，紧接着是一个RELU，一个max pooling（步长为2），用来降采样，每次降采样我们都将feature channel扩大一倍，从64、128、256、512、1024。两个3x3的卷积核之后跟一个2x2的最大化池化层，缩小图片的分辨率。（其实也可以将收缩路径看成是一个**“编码器Ecoder”**，只是作者不这么叫）

**（2）扩展路径（expanding path）**包含一个上采样（2x2上卷积），将图像大小扩大一倍，然后再使用普通的3x3卷积核，再将通道数feature channel缩小一倍，从1024、512、256、128、64（其实也可以将拓展路径看成是一个**“解码器Decoder”**，只是作者不这么叫）。

**（3）裁剪crop并且跨层连接**

上面的结构其实如果是水平方向展开，就是一个**“编码-解码”**的这样一个结构，但是有两个地方是需要注意的：

- 第一：编码解码的feature map大小是**不对称的；**

- 第二：我需要使用跨层连接来提高特征的利用率，在上采样的时候利用前面低层的特征信息；

鉴于这两个点，U-Net依然沿袭了FCN网络的处理方式——进行特征融合，但是与FCN不同的是，融合的方式有所差别：
- 第一：由于特征图feature map不对称，**所以要能够融合需要将收缩路径中的feature map进行裁剪crop，大小一样了才能融合；**
- 第二：特征融合方式是**“拼接“**，U-net采用将特征在channel维度拼接在一起，形成更厚的channel。而FCN融合时使用的对应点相加，并不形成更厚的特征。一个是concat，一个是add。



##### 输入输出不一致怎么解决？

观察Unet网络，可以发现我们的输入图像和输出的图像的尺寸是不一致的（输入572 * 572，但是输出388 *  388），那么我们的网络该如何选择GT进行损失的计算呢（Unet中，原始的paper是一个像素的二分类问题，采用交叉熵损失）。

![img](https://img-blog.csdnimg.cn/20190423175747474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3ODI1NDUx,size_16,color_FFFFFF,t_70)

在上图中，先看左侧图像，如果我的原始图像大小是红色方框这么大，要分割的位置是图像中的黄色方框，将其输入到U-Net网络中之后，经过运算，由于输入和输出大小不相等，所以得到的输出图像必然小于原始图像，那得到的右边的那个黄色框框自然也是小于原始图像的黄色框框的。

为了保证输出图像跟我的输入图像一样大，我还需要先将输入图像进行预处理，即我将输入图像（红色方框）先变大一些，记得到上图中的左边的那个大图。具体的操作是：就是对图像做镜像操作，四个边做镜像往外翻一下就扩大了图像。然后把扩大后的图像输入网络，然后输出后图像的大小刚好和输入图像的大小（即红色框框）大小一样，输出中要分割的黄色框框也和输入的图像中的黄色框框大小是一样的，这样就可以进行loss计算了，在计算loss的时候，其实只考虑了原始的输入图像，即红色方框，镜像翻转的那部分是不参与loss计算的，通过扩大输入图片，这样，输入和输出像素之间相对的一一对应关系，就能够计算loss了。

> 内容来自：https://blog.csdn.net/qq_27825451/article/details/89470972

### 

