# 优化器

参考知乎文章：https://zhuanlan.zhihu.com/p/489706243

优化器具体可以被分为两类，第一类是非自适应优化器，第二类是自适应的优化器。前者在整个优化过程当中，学习率不变，或者按照时间变化，常见的例如`SGD`，`SGDM`。后者的学习率随着梯度自适应变换，并且尽可能去消去给定全局学习率的影响，这叫做自适应优化器，常见如`Adagrad`，`RMSprop`，`Adam`等。

## 非自适应优化器：

### SGD

原始的SGD每次只随机选择一个样本进行前向传播，然后反向计算梯度，原始SGD的速度非常快。但是缺点也是十分明显的，我们的损失函数可能会有很大的震荡，甚至难以收敛。

和SGD对应的是MBGD（Mini-batch gradient descent），或者有的时候，我们就把MBGD叫做SGD，MBGD每次利用一整个batch的样本，即利用n个样本进行计算。这样的好处是可以降低训练更新时候的方差，也使得收敛更加稳定，另外一方面，像pytorch等框架都对矩阵运算有各种优化，速度也不错。

mini-bactch也有一些缺点：如何选择合适的batch是一个问题，再者mini-batch依旧会陷入局部最小值，或者鞍点的位置。

### SGDM

其全称为SGD with momentum，于1986年提出，具体的公式为：

![image-20220731150527129](D:\OneDrive\Documents\cv-point\imgs\opt-1.png)

其中 λ 和 η 均为超参数，θ 为模型的参数，具体的参数更新的公式为：

![image-20220731150729734](D:\OneDrive\Documents\cv-point\imgs\opt-2.png)

如果我们将 v 初始化为0，那么有：

![image-20220731150843088](D:\OneDrive\Documents\cv-point\imgs\opt-3.png) 

依次类推，当我们计算到 n 趋向于无穷的时候，会发现之前的梯度影响会越小。这样的方式，综合了之前的梯度的积累，会使得梯度方向不变的方向上，下降速度变快，在梯度方向改变的方向上，下降速度变慢。

